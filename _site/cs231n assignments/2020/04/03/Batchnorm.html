<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="https://gmpg.org/xfn/11" rel="profile" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1" />

  <title>
    
      Batch Normalization -- CS231n Exercise &middot; UR Machine Learning Blog
    
  </title>

  


  <!-- CSS -->
  <link rel="stylesheet" href="/urmlblog/assets/css/main.css" />
  

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface" />

  <!-- Icons -->
<!--  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/urmlblog/favicon.png" />
<link rel="shortcut icon" href="/urmlblog/favicon.ico" />-->

  <!-- RSS -->
<!--  <link rel="alternate" type="application/rss+xml" title="RSS" href="/urmlblog/feed.xml" />-->

  <!-- Additional head bits without overriding original head -->
</head>


  <script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "all" } }
  });
  MathJax.Hub.Config({ TeX: { extensions: ["color.js"] }});
<!--  MathJax.Hub.Config({-->
<!--    jax: ["input/TeX","output/HTML-CSS"],-->
<!--    displayAlign: "left"-->
<!--  });-->
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/urmlblog/favicon.png" />
<link rel="shortcut icon" href="/urmlblog/favicon.ico" />

  <body class="post">

    <div id="sidebar">
  <header>
    <div class="site-title">
      <a href="/urmlblog/">
        
          <span class="back-arrow icon"><svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
  <path d="M0 0h24v24H0z" fill="none"/>
  <path d="M20 11H7.83l5.59-5.59L12 4l-8 8 8 8 1.41-1.41L7.83 13H20v-2z"/>
</svg></span>
        
        UR Machine Learning Blog
      </a>
    </div>
    <p class="lead">Data Scientist at City of Edmonton</p>
  </header>
  <nav id="sidebar-nav-links">
  
    <a class="home-link "
        href="/urmlblog/">Home</a>
  
  

  

  


  
    
  

  
    
      <a class="page-link "
          href="/urmlblog/about.html">About</a>
    
  

  
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  
    
  

  
    
  


  


  
    
  

  
    
  

  
    
      <a class="category-link "
          href="/urmlblog/category/categories.html">Categories</a>
    
  

  
    
  

  

  
    
  

  
    
  

  

  

  

  

  
    
  

  
    
  


  <!-- Optional additional links to insert in sidebar nav -->
</nav>


  

  <nav id="sidebar-icon-links">
  

<!--  <a id="subscribe-link"-->
<!--     class="icon" title="Subscribe" aria-label="Subscribe"-->
<!--     href="/urmlblog/feed.xml">-->
<!--    <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <circle cx="6.18" cy="17.82" r="2.18"/>
    <path d="M4 4.44v2.83c7.03 0 12.73 5.7 12.73 12.73h2.83c0-8.59-6.97-15.56-15.56-15.56zm0 5.66v2.83c3.9 0 7.07 3.17 7.07 7.07h2.83c0-5.47-4.43-9.9-9.9-9.9z"/>
</svg>-->
<!--  </a>-->

  
  
  
  

  <a id="linkedin-link"
       title="Linkedin" aria-label="linkedin" class="icon" target="_blank"
       href="https://www.linkedin.com/in/usman-rizwan-data-magic/">
      <?xml version="1.0" encoding="UTF-8"?>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" width="24pt" height="24pt" viewBox="0 0 24 24" version="1.1">
<defs>
<filter id="alpha" filterUnits="objectBoundingBox" x="0%" y="0%" width="100%" height="100%">
  <feColorMatrix type="matrix" in="SourceGraphic" values="0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0"/>
</filter>
<mask id="mask0">
  <g filter="url(#alpha)">
<rect x="0" y="0" width="24" height="24" style="fill:rgb(0%,0%,0%);fill-opacity:0.780392;stroke:none;"/>
  </g>
</mask>
<linearGradient id="linear0" gradientUnits="userSpaceOnUse" x1="-808.8727" y1="199.8918" x2="-860.5692" y2="49.2029" gradientTransform="matrix(-0.0554531,0,0,-0.0554531,-38.267222,8.831372)">
<stop offset="0" style="stop-color:rgb(100%,100%,100%);stop-opacity:1;"/>
<stop offset="1" style="stop-color:rgb(100%,100%,100%);stop-opacity:0;"/>
</linearGradient>
<clipPath id="clip1">
  <rect width="24" height="24"/>
</clipPath>
<g id="surface6" clip-path="url(#clip1)">
<path style=" stroke:none;fill-rule:nonzero;fill:url(#linear0);" d="M 17.195312 1.925781 L 6.804688 1.925781 C 4.054688 1.925781 1.839844 4.160156 1.839844 6.929688 L 1.839844 17.070312 C 1.929688 19.234375 2.273438 17.863281 2.921875 15.46875 C 3.679688 12.683594 6.148438 10.246094 9.152344 8.421875 C 11.445312 7.03125 14.011719 6.140625 18.683594 6.054688 C 21.335938 6.007812 21.101562 2.617188 17.195312 1.925781 Z M 17.195312 1.925781 "/>
</g>
</defs>
<g id="surface1">
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(11.372549%,54.901961%,70.980392%);fill-opacity:1;" d="M 19.6875 0.984375 L 4.3125 0.984375 C 2.472656 0.984375 0.984375 2.472656 0.984375 4.3125 L 0.984375 19.6875 C 0.984375 21.527344 2.472656 23.015625 4.3125 23.015625 L 19.6875 23.015625 C 21.527344 23.015625 23.015625 21.527344 23.015625 19.6875 L 23.015625 4.3125 C 23.015625 2.472656 21.527344 0.984375 19.6875 0.984375 Z M 19.6875 0.984375 "/>
<use xlink:href="#surface6" mask="url(#mask0)"/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;" d="M 7.648438 19.476562 L 7.648438 9.515625 L 4.339844 9.515625 L 4.339844 19.476562 Z M 5.996094 8.15625 C 7.148438 8.15625 7.867188 7.390625 7.867188 6.433594 C 7.847656 5.457031 7.148438 4.710938 6.015625 4.710938 C 4.882812 4.710938 4.144531 5.457031 4.144531 6.433594 C 4.144531 7.390625 4.863281 8.15625 5.972656 8.15625 Z M 5.996094 8.15625 "/>
<path style=" stroke:none;fill-rule:nonzero;fill:rgb(100%,100%,100%);fill-opacity:1;" d="M 9.484375 19.476562 L 12.792969 19.476562 L 12.792969 13.914062 C 12.792969 13.617188 12.816406 13.320312 12.902344 13.105469 C 13.144531 12.511719 13.6875 11.894531 14.601562 11.894531 C 15.800781 11.894531 16.28125 12.808594 16.28125 14.148438 L 16.28125 19.476562 L 19.589844 19.476562 L 19.589844 13.765625 C 19.589844 10.703125 17.957031 9.28125 15.777344 9.28125 C 13.992188 9.28125 13.207031 10.277344 12.773438 10.960938 L 12.792969 10.960938 L 12.792969 9.515625 L 9.484375 9.515625 C 9.527344 10.449219 9.484375 19.476562 9.484375 19.476562 Z M 9.484375 19.476562 "/>
</g>
</svg>

    </a>

    <a id="github-link"
       title="Github" aria-label="github" class="icon" target="_blank"
       href="https://github.com/usmanr149/">
      <svg version="1.1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 28" height="24" width="28"><path d="M12 2c6.625 0 12 5.375 12 12 0 5.297-3.437 9.797-8.203 11.391-0.609 0.109-0.828-0.266-0.828-0.578 0-0.391 0.016-1.687 0.016-3.297 0-1.125-0.375-1.844-0.812-2.219 2.672-0.297 5.484-1.313 5.484-5.922 0-1.313-0.469-2.375-1.234-3.219 0.125-0.313 0.531-1.531-0.125-3.187-1-0.313-3.297 1.234-3.297 1.234-0.953-0.266-1.984-0.406-3-0.406s-2.047 0.141-3 0.406c0 0-2.297-1.547-3.297-1.234-0.656 1.656-0.25 2.875-0.125 3.187-0.766 0.844-1.234 1.906-1.234 3.219 0 4.594 2.797 5.625 5.469 5.922-0.344 0.313-0.656 0.844-0.766 1.609-0.688 0.313-2.438 0.844-3.484-1-0.656-1.141-1.844-1.234-1.844-1.234-1.172-0.016-0.078 0.734-0.078 0.734 0.781 0.359 1.328 1.75 1.328 1.75 0.703 2.141 4.047 1.422 4.047 1.422 0 1 0.016 1.937 0.016 2.234 0 0.313-0.219 0.688-0.828 0.578-4.766-1.594-8.203-6.094-8.203-11.391 0-6.625 5.375-12 12-12zM4.547 19.234c0.031-0.063-0.016-0.141-0.109-0.187-0.094-0.031-0.172-0.016-0.203 0.031-0.031 0.063 0.016 0.141 0.109 0.187 0.078 0.047 0.172 0.031 0.203-0.031zM5.031 19.766c0.063-0.047 0.047-0.156-0.031-0.25-0.078-0.078-0.187-0.109-0.25-0.047-0.063 0.047-0.047 0.156 0.031 0.25 0.078 0.078 0.187 0.109 0.25 0.047zM5.5 20.469c0.078-0.063 0.078-0.187 0-0.297-0.063-0.109-0.187-0.156-0.266-0.094-0.078 0.047-0.078 0.172 0 0.281s0.203 0.156 0.266 0.109zM6.156 21.125c0.063-0.063 0.031-0.203-0.063-0.297-0.109-0.109-0.25-0.125-0.313-0.047-0.078 0.063-0.047 0.203 0.063 0.297 0.109 0.109 0.25 0.125 0.313 0.047zM7.047 21.516c0.031-0.094-0.063-0.203-0.203-0.25-0.125-0.031-0.266 0.016-0.297 0.109s0.063 0.203 0.203 0.234c0.125 0.047 0.266 0 0.297-0.094zM8.031 21.594c0-0.109-0.125-0.187-0.266-0.172-0.141 0-0.25 0.078-0.25 0.172 0 0.109 0.109 0.187 0.266 0.172 0.141 0 0.25-0.078 0.25-0.172zM8.937 21.438c-0.016-0.094-0.141-0.156-0.281-0.141-0.141 0.031-0.234 0.125-0.219 0.234 0.016 0.094 0.141 0.156 0.281 0.125s0.234-0.125 0.219-0.219z"></path>
</svg>

    </a>

  
    <a id="tags-link"
       class="icon"
       title="Tags" aria-label="Tags"
       href="/urmlblog/tags.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
    </a>
  

  
    <a id="search-link"
       class="icon"
       title="Search" aria-label="Search"
       href="/urmlblog/search.html">
      <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M15.5 14h-.79l-.28-.27C15.41 12.59 16 11.11 16 9.5 16 5.91 13.09 3 9.5 3S3 5.91 3 9.5 5.91 16 9.5 16c1.61 0 3.09-.59 4.23-1.57l.27.28v.79l5 4.99L20.49 19l-4.99-5zm-6 0C7.01 14 5 11.99 5 9.5S7.01 5 9.5 5 14 7.01 14 9.5 11.99 14 9.5 14z"/>
    <path d="M0 0h24v24H0z" fill="none"/>
</svg>
    </a>
  

  <!-- Optional additional links to insert for icons links -->
</nav>

  <!--<p>-->
<!--  &copy; 2022.-->
<!--  <a href="/urmlblog/LICENSE.md">MIT License.</a>-->
<!--</p>-->

</div>

    <main class="container">
      <header>
  <h1 class="post-title">Batch Normalization -- CS231n Exercise</h1>
</header>
<div class="content">
  <div class="post-meta">
  <span class="post-date">03 Apr 2020</span>
  <span class="post-categories">
    
      &bull;

      
      
      

      
        CS231n assignments
      
    
  </span>
</div>


  <div class="post-body">
    <p class="message">
I am currently working my way through the lectures for 
<a href="https://www.youtube.com/watch?v=vT1JzLTH4G4&amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;index=1" target="_blank">CS231n: Convolutional Neural Networks for Visual Recognition</a>.
I will post my solutions <a href="https://usmanr149.github.io/urmlblog/" target="_blank">here</a>.
</p>

<h2 id="batch-normalization--forward">Batch Normalization – forward</h2>

<p>Batch normalization is a really interesting technique that reduces internal covariate shift and 
accelerates the training of deep neural nets. Read the <a href="https://arxiv.org/abs/1502.03167" target="_blank">original paper</a> 
for more details.</p>

<h2>A word about notation</h2>
<p class="message">
To signify sums over each column in a <i>N x D</i> matrix (<i>Z</i>) I will use the notation

$$
\sum^{axis=0}Z
$$ 
and the result will be a <i>D</i> dimensional vector. For sums over each row I will use 
$$
\sum^{axis=1}Z
$$ 

and the result will be a <i>N</i> dimensional vector.
</p>

<p>Batch normalization is applied across feature axis. For e.g. if we have a batch of three samples and 
each sample has five dimensions as follows</p>

<p><img src="/urmlblog/images/FC_NN/Batchnorm_along_feature_axis.png" alt="_config.yml" />
<em>In batch normalization, the normalization is done across feature axis.</em></p>

<p>We can define the mean and variance across the features axis as follows</p>

\[\begin{align}
\mu_{k} = \frac{1}{3}\sum_{i=0}^{2} x_{i,k} 
\end{align}\]

\[\begin{align}\label{eqn:sigma}
\sigma_{k}^2 = \frac{1}{3}\sum_{i=0}^{2}(x_{i,k} - \mu_{k})^2
\end{align}\]

<p>and the batch normalized matrix will look as follows</p>

\[\begin{align} \label{eqn:xhat}
\hat{x} = 
\begin{pmatrix}
\frac{x_{0,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} &amp; \frac{x_{0,1} - \mu_{1}}{\sqrt{\sigma_{1}^2 + \epsilon}} &amp; 
\frac{x_{0,2} - \mu_{2}}{\sqrt{\sigma_{2}^2 + \epsilon}} &amp; \frac{x_{0,3} - \mu_{3}}{\sqrt{\sigma_{3}^2 + \epsilon}} &amp; \frac{x_{0,4} - \mu_{4}}{\sqrt{\sigma_{4}^2 + \epsilon}} \\
\frac{x_{1,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} &amp; \frac{x_{1,1} - \mu_{1}}{\sqrt{\sigma_{1}^2 + \epsilon}} &amp; 
\frac{x_{1,2} - \mu_{2}}{\sqrt{\sigma_{2}^2 + \epsilon}} &amp; \frac{x_{1,3} - \mu_{3}}{\sqrt{\sigma_{3}^2 + \epsilon}} &amp; \frac{x_{1,4} - \mu_{4}}{\sqrt{\sigma_{4}^2 + \epsilon}} \\
\frac{x_{2,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} &amp; \frac{x_{2,1} - \mu_{1}}{\sqrt{\sigma_{1}^2 + \epsilon}} &amp; 
\frac{x_{2,2} - \mu_{2}}{\sqrt{\sigma_{2}^2 + \epsilon}} &amp; \frac{x_{2,3} - \mu_{3}}{\sqrt{\sigma_{3}^2 + \epsilon}} &amp; \frac{x_{2,4} - \mu_{4}}{\sqrt{\sigma_{4}^2 + \epsilon}}
\end{pmatrix}
\end{align}\]

<p>Where \(\epsilon\) is a very small value added to the denominator to prevent division by zero. 
The normalized activation is scaled and shifted:</p>

\[\begin{align}\label{eqn:bn}
y = \gamma \hat{x} + \beta
\end{align}\]

<p>where</p>

\[\gamma = \begin{pmatrix}
\gamma_0 &amp; \gamma_1 &amp; \gamma_2 &amp; \gamma_3 &amp; \gamma_4
\end{pmatrix}\]

<p>and</p>

\[\beta = \begin{pmatrix}
\beta_0 &amp; \beta_1 &amp; \beta_2 &amp; \beta_3 &amp; \beta_4
\end{pmatrix}\]

<p>The parameters \(\gamma\) and \(\beta\) are learned along with the original model parameters. They help restore 
the representational power of the network. This methodology can applied in python as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">batchnorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">bn_param</span><span class="p">):</span>
    <span class="n">mode</span> <span class="o">=</span> <span class="n">bn_param</span><span class="p">[</span><span class="s">'mode'</span><span class="p">]</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">bn_param</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'eps'</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
    <span class="n">momentum</span> <span class="o">=</span> <span class="n">bn_param</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'momentum'</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">)</span>

    <span class="n">N</span><span class="p">,</span> <span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">running_mean</span> <span class="o">=</span> <span class="n">bn_param</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'running_mean'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="n">running_var</span> <span class="o">=</span> <span class="n">bn_param</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'running_var'</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">))</span>

    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">'train'</span><span class="p">:</span>
        <span class="n">sample_mean</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">running_mean</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">running_mean</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">sample_mean</span>

        <span class="n">sample_var</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">running_var</span> <span class="o">=</span> <span class="n">momentum</span> <span class="o">*</span> <span class="n">running_var</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span><span class="p">)</span> <span class="o">*</span> <span class="n">sample_var</span>

        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">sample_mean</span><span class="p">)</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sample_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">*</span><span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>

        <span class="n">cache</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'beta'</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'x_norm'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_norm</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'sample_mean'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_mean</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'sample_var'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_var</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'eps'</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="n">cache</span><span class="p">[</span><span class="s">'N'</span><span class="p">],</span> <span class="n">cache</span><span class="p">[</span><span class="s">'D'</span><span class="p">]</span> <span class="o">=</span> <span class="n">N</span><span class="p">,</span> <span class="n">D</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s">'test'</span><span class="p">:</span>
        <span class="n">x_norm</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">running_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">running_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>

     <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">ValueError</span><span class="p">(</span><span class="s">'Invalid forward batchnorm mode "%s"'</span> <span class="o">%</span> <span class="n">mode</span><span class="p">)</span>

    <span class="c1"># Store the updated running means back into bn_param
</span>    <span class="n">bn_param</span><span class="p">[</span><span class="s">'running_mean'</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_mean</span>
    <span class="n">bn_param</span><span class="p">[</span><span class="s">'running_var'</span><span class="p">]</span> <span class="o">=</span> <span class="n">running_var</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>

<h2 id="batch-normalization--backward">Batch Normalization – backward</h2>

<p>Calculating batch normalization via the computation graph is quite tedious. Review the details of 
matrix multiplication bacward propogation in the <a href="http://cs231n.stanford.edu/handouts/linear-backprop.pdf" target="_blank">lecture 4 handouts</a>
to better understand the derivation given below.</p>

<p>Lets start with the computation graph of the forward pass first and then go through the backward pass.</p>

<p><img src="/urmlblog/images/FC_NN/Batchnorm_Backward.png" alt="_config.yml" />
<em>Forward pass of the batch normalization.</em></p>

<p>Now lets start the backward propogation. We assume that we have recieved a \(N\times D\) matrix 
\(\frac{\partial L}{\partial y}\) from upstream. So lets calculate</p>

\[\frac{\partial L}{\partial \beta}, \text{This will be a } D \text{ dimensional vector.} \nonumber\\
\frac{\partial L}{\partial g}, \text{This will be a } N \times D \text{ matrix.} \nonumber\]

<p><img src="/urmlblog/images/FC_NN/Batchnorm_Backward_1.png" alt="_config.yml" /></p>

\[\begin{align}\label{eqn:dbeta}
\frac{\partial L}{\partial \beta} &amp;= \frac{\partial L}{\partial y}\frac{\partial y}{\partial \beta} \nonumber \\
&amp;= \sum^{axis=0}\left( \frac{\partial L}{\partial y} \right), \text{This will be a } D \text{ dimensional vector.}
\end{align}\]

<p>and</p>

\[\begin{align}\label{eqn:dg}
\frac{\partial L}{\partial g} &amp;= \frac{\partial L}{\partial y}\frac{\partial y}{\partial g} \nonumber \\
&amp;= \frac{\partial L}{\partial y}\frac{\partial (g + \beta) }{\partial g} \nonumber \\
&amp;= \frac{\partial L}{\partial y}, \text{This will be a } N \times D \text{ matrix.}
\end{align}\]

<p>Sliding backward</p>

<p><img src="/urmlblog/images/FC_NN/Batchnorm_Backward_2.png" alt="_config.yml" /></p>

\[\begin{align}\label{eqn:dgamma}
\frac{\partial L}{\partial \gamma} &amp;= \frac{\partial L}{\partial g}\frac{\partial g}{\partial \gamma} \nonumber \\
&amp;= \frac{\partial L}{\partial g}\frac{\partial (\gamma f)}{\partial \gamma} \nonumber \\
&amp;= \frac{\partial L}{\partial g}f \nonumber \\
&amp;= \sum^{axis=0}\left(\frac{\partial L}{\partial y}f\right), \text{This will be a } D \text{ dimensional vector.}
\end{align}\]

<p>and</p>

\[\begin{align}\label{eqn:df}
\frac{\partial L}{\partial f} &amp;= \frac{\partial L}{\partial g}\frac{\partial g}{\partial f} \nonumber \\
&amp;= \frac{\partial L}{\partial g}\frac{\partial (\gamma f)}{\partial f} \nonumber \\
&amp;= \frac{\partial L}{\partial g}\gamma , \text{This will be a } N \times D \text{ matrix.}
\end{align}\]

<h3 id="variance-stream">Variance Stream</h3>

<p><img src="/urmlblog/images/FC_NN/Batchnorm_Backward_3.png" alt="_config.yml" /></p>

<p>As we slide backward the stream gets broken into two. So lets first follow the variance stream back to 
its origin.</p>

\[\begin{align}
\frac{\partial L}{\partial e} &amp;= \frac{\partial L}{\partial f}\frac{\partial f}{\partial e} \nonumber \\
&amp;=\frac{\partial L}{\partial y}\gamma\frac{\partial (be)}{\partial e} \nonumber \\
&amp;=\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) , \text{This will be a } D \text{ dimensional vector.}
\end{align}\]

\[\begin{align}
\frac{\partial L}{\partial d} &amp;= \frac{\partial L}{\partial e} \frac{\partial e}{\partial d} \nonumber \\
&amp;=\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{\partial (\frac{1}{d})}{\partial d} \nonumber \\
&amp;=-\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}} , \text{This will be a } D \text{ dimensional vector.}
\end{align}\]

\[\begin{align}
\frac{\partial L}{\partial c} &amp;= \frac{\partial L}{\partial d}\frac{\partial d}{\partial c} \nonumber \\
&amp; = -\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}}\frac{\partial \sqrt{c}}{\partial c} \nonumber \\
&amp; = -\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}}\frac{1}{2}\frac{1}{\sqrt{c}} , \text{This will be a } D \text{ dimensional vector.}
\end{align}\]

\[\begin{align}
\frac{\partial L}{\partial \sigma^2} &amp;= \frac{\partial L}{\partial c}\frac{\partial c}{\partial \sigma^2} \nonumber \\
&amp; = -\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}}\frac{1}{2}\frac{1}{\sqrt{c}} \frac{\partial (\sigma^2 + \epsilon)}{\partial \sigma^2} \nonumber \\
&amp; = -\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}}\frac{1}{2}\frac{1}{\sqrt{c}}, \text{This will be a } D \text{ dimensional vector.}
\end{align}\]

\[\begin{align}
\frac{\partial L}{\partial x} &amp;= \frac{\partial L}{\partial \sigma^2}\frac{\partial \sigma^2}{\partial x}, \text{This will be a } N \times D \text{ matrix.}
\end{align}\]

<p>This needs to be solved in an element-wise fashion. Lets focus on $x_{0,0}$ to get a sense of the what
the result looks like.</p>

\[\begin{align}
\frac{\partial L}{\partial x_{0,0}} = \frac{\partial L}{\partial \sigma^2}\frac{\partial \sigma^2}{\partial x_{0,0}} \\
\end{align}\]

<p>Note that \(\frac{\partial L}{\partial \sigma^2}\) and \(\frac{\partial \sigma_0^2}{\partial x_{0,0}}\) 
are $D$ dimensional vectors.</p>

\[\begin{align}
\frac{\partial \sigma^2}{\partial x_{0,0}} = 
\begin{pmatrix}
\frac{\partial \sigma_0^2}{\partial x_{0,0}} &amp; \frac{\partial \sigma_1^2}{\partial x_{0,0}} &amp; 
\dots  &amp; \frac{\partial \sigma_{D-2}^2}{\partial x_{0,0}} &amp; 
\frac{\partial \sigma_{D-1}^2}{\partial x_{0,0}}
\end{pmatrix}
\end{align}\]

<p>Similar to Eq. \ref{eqn:sigma}, we know that</p>

\[\begin{align}
\sigma_{k}^2 = \frac{1}{N}\sum_{i=0}^{N-1}(x_{i,k} - \mu_{k})^2
\end{align}\]

<p>Using the equation above,</p>

\[\begin{align}
\frac{\partial \sigma^2}{\partial x_{0,0}} = 
\begin{pmatrix}
\frac{2}{N} (x_{0,0} - \mu_0) &amp; 0 &amp; \dots &amp; 0 &amp; 0
\end{pmatrix}
\end{align}\]

<p>Therefore</p>

\[\begin{align}
\frac{\partial L}{\partial x_{0,0}} &amp;= \frac{\partial L}{\partial \sigma^2} \frac{\partial \sigma^2}{\partial x_{0,0}} \nonumber \\
&amp; = \left(\frac{\partial L}{\partial \sigma^2}\right)_{0} \frac{2}{N} (x_{0,0} - \mu_0)
\end{align}\]

<p>Rewiriting in a more readable format</p>

\[\begin{align}
\frac{\partial L}{\partial x_{0,0}} 
&amp; = \frac{2}{N} (x_{0,0} - \mu_0) \left(\frac{\partial L}{\partial \sigma^2}\right)_{0}
\end{align}\]

<p>We can similarly show that</p>

\[\begin{align}\label{eqn:grad_L_x}
\frac{\partial L}{\partial x_{k,j}} 
&amp; = \frac{2}{N} (x_{k,j} - \mu_j) \left(\frac{\partial L}{\partial \sigma^2}\right)_{j}
\end{align}\]

<p>where \(k \in N-1\) and \(j \in D-1\). We can rewrite the equation above in matrix form</p>

\[\begin{align}
\frac{\partial L}{\partial x}  &amp; = \frac{2}{N} (x - \mu) \frac{\partial L}{\partial \sigma^2}
\end{align}\]

<p>Taking the result for \(\frac{\partial L}{\partial \sigma^2}\) into account.</p>

\[\begin{align}
\frac{\partial L}{\partial x}  &amp; = \frac{2}{N} (x - \mu) \left(-\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}}\frac{1}{2}\frac{1}{\sqrt{c}}\right) \nonumber \\
&amp; = -\frac{2}{N} (x - \mu) \left(\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}}\frac{1}{2}\frac{1}{\sqrt{c}}\right)
\end{align}\]

<p>This is the derivative from just one stream, now we need to follow the other stream.</p>

<h3 id="the-other-stream">The Other Stream</h3>

<p><img src="/urmlblog/images/FC_NN/Batchnorm_Backward_5.png" alt="_config.yml" /></p>

\[\begin{align}
\frac{\partial L}{\partial b} &amp;= \frac{\partial L}{\partial f}\frac{\partial f}{\partial b} \nonumber \\
&amp;=\frac{\partial L}{\partial y}\gamma \frac{\partial (be)}{\partial b} \nonumber \\
&amp;=\frac{\partial L}{\partial y}\gamma e \label{eqn:grad_L_b}, \text{This will be a } N \times D \text{ matrix.}
\end{align}\]

\[\begin{align}
\frac{\partial L}{\partial \mu} &amp;= \frac{\partial L}{\partial b}\frac{\partial b}{\partial \mu}, \text{This will be a } D \text{ dimensional vector.} \\
\end{align}\]

<p>This needs to be broken down into its elements</p>

\[\begin{align}
\frac{\partial b}{\partial \mu_0} &amp;=
\begin{pmatrix}
\frac{\partial(x_{0,0} - \mu_0)}{\partial \mu_{0}} &amp; \frac{\partial(x_{0,1} - \mu_1)}{\partial \mu_{0}} 
&amp; \dots &amp; \frac{\partial(x_{0,D-2} - \mu_{D-2})}{\partial \mu_{0}} &amp; \frac{\partial(x_{0,D-1} - \mu_{D-1})}{\partial \mu_{0}} \\
\frac{\partial(x_{1,0} - \mu_0)}{\partial \mu_{0}} &amp; \frac{\partial(x_{1,1} - \mu_1)}{\partial \mu_{0}} 
&amp; \dots &amp; \frac{\partial(x_{1,D-2} - \mu_{D-2})}{\partial \mu_{0}} &amp; \frac{\partial(x_{1,D-1} - \mu_{D-1})}{\partial \mu_{0}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
\frac{\partial(x_{N-1,0} - \mu_0)}{\partial \mu_{0}} &amp; \frac{\partial(x_{N-1,1} - \mu_1)}{\partial \mu_{0}} 
&amp; \dots &amp; \frac{\partial(x_{N-1,D-2} - \mu_{D-2})}{\partial \mu_{0}} &amp; \frac{\partial(x_{N-1,D-1} - \mu_{D-1})}{\partial \mu_{0}}
\end{pmatrix} \nonumber \\
&amp;=\begin{pmatrix}
-1 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
-1 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
-1 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
\end{pmatrix}
\end{align}\]

<p>Note that even though \(\mu_0\) is dependent on \(x_{j,0}\) (where \(j \in D-1\)) we don’t need to take that 
into account here as it will come into play in the other stream backprop calculated in the next section.</p>

\[\begin{align}
\frac{\partial b}{\partial \mu_0} &amp;= -\sum_{i=0}^{N-1}\left(\frac{\partial L}{\partial b}\right)_{i,0}
\end{align}\]

<p>and</p>

\[\begin{align}\label{eq:grad_L_mu}
\frac{\partial L}{\partial \mu} &amp;=-\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma e\right)
\end{align}\]

\[\begin{align}
\frac{\partial L}{\partial x} &amp;= \frac{\partial L}{\partial \mu}\frac{\partial \mu}{\partial x}, \text{This will be a } N \times D \text{ matrix.}
\end{align}\]

<p>This also need to be solved in an element wise fashion</p>

\[\begin{align}
\frac{\partial \mu}{\partial x_{0,0}} &amp;= \begin{pmatrix}
\frac{\partial \mu_0}{\partial x_{0,0}} &amp; \frac{\partial \mu_1}{\partial x_{0,0}} &amp; 
\dots &amp; \frac{\partial \mu_{D-2}}{\partial x_{0,0}} &amp; \frac{\partial \mu_{D-1}}{\partial x_{0,0}}
\end{pmatrix} \nonumber \\
&amp;= \begin{pmatrix}
1/N &amp; 0 &amp; \dots &amp; 0 &amp; 0
\end{pmatrix}
\end{align}\]

<p>Therefore</p>

\[\begin{align}
\frac{\partial L}{\partial x_{0,0}} &amp;= \frac{1}{N}\left( \frac{\partial L}{\partial \mu} \right)_{0,0}
\end{align}\]

<p>It can be similary shown that</p>

\[\begin{align}
\frac{\partial \mu}{\partial x_{k,0}}
&amp;= \begin{pmatrix}
1/N &amp; 0 &amp; \dots &amp; 0 &amp; 0
\end{pmatrix}
\\
\frac{\partial \mu}{\partial x_{k,1}}
&amp;= \begin{pmatrix}
0 &amp; 1/N &amp; \dots  &amp; 0 &amp; 0
\end{pmatrix}
\\
\frac{\partial \mu}{\partial x_{k,D-2}}
&amp;= \begin{pmatrix}
0 &amp; 0 &amp; \dots &amp; 1/N &amp; 0
\end{pmatrix}
\\
\frac{\partial \mu}{\partial x_{k,D-1}}
&amp;= \begin{pmatrix}
0 &amp; 0 &amp; \dots &amp; 0 &amp; 1/N
\end{pmatrix}
\end{align}\]

<p>Therefore</p>

\[\begin{align}
\frac{\partial L}{\partial x} &amp;= \frac{1}{N}\left(\frac{\partial L}{\partial \mu}\right)
\end{align}\]

<p>and using Eq. \ref{eq:grad_L_mu}</p>

\[\begin{align}\label{eqn:grad_l_x_stream2}
\frac{\partial L}{\partial x} &amp;= -\frac{1}{N}\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma e\right) \\
\end{align}\]

<h2 id="the-last-stream">The Last Stream</h2>

<p><img src="/urmlblog/images/FC_NN/Batchnorm_Backward_4.png" alt="_config.yml" /></p>

\[\begin{align}
\frac{\partial L}{\partial x} &amp;= \frac{\partial L}{\partial b}\frac{\partial b}{\partial x}, \text{This will be a } N \times D \text{ matrix.} \\
\end{align}\]

<p>This also needs to be looked at in an element wise fashion</p>

\[\begin{align}
\frac{\partial b}{\partial x_{0,0}} &amp;=
\begin{pmatrix}
\frac{\partial(x_{0,0} - \mu_0)}{\partial x_{0,0}} &amp; \frac{\partial(x_{0,1} - \mu_1)}{\partial x_{0,0}} 
&amp; \dots &amp; \frac{\partial(x_{0,D-2} - \mu_{D-2})}{\partial x_{0,0}} &amp; \frac{\partial(x_{0,D-1} - \mu_{D-1})}{\partial x_{0,0}} \\
\frac{\partial(x_{1,0} - \mu_0)}{\partial x_{0,0}} &amp; \frac{\partial(x_{1,1} - \mu_1)}{\partial x_{0,0}} 
&amp; \dots &amp; \frac{\partial(x_{1,D-2} - \mu_{D-2})}{\partial x_{0,0}} &amp; \frac{\partial(x_{1,D-1} - \mu_{D-1})}{\partial \mu_{0}} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
\frac{\partial(x_{N-1,0} - \mu_0)}{\partial x_{0,0}} &amp; \frac{\partial(x_{N-1,1} - \mu_1)}{\partial x_{0,0}} 
&amp; \dots &amp; \frac{\partial(x_{N-1,D-2} - \mu_{D-2})}{\partial x_{0,0}} &amp; \frac{\partial(x_{N-1,D-1} - \mu_{D-1})}{\partial x_{0,0}}
\end{pmatrix} \nonumber \\
&amp;=\begin{pmatrix}
1 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \dots &amp; 0 &amp; 0 \\
\end{pmatrix}
\end{align}\]

<p>Note that even though \(x_{0,0}\) is used to calculate \(\mu_0\) we don’t need to worry about that here as 
we already took that into account when deriving Eq. \ref{eqn:grad_l_x_stream2}. Using the equation above we 
can see that</p>

\[\begin{align}
\frac{\partial L}{\partial x_{0,0}} &amp;= \left(\frac{\partial L}{\partial b}\right)_{0,0} \\
\end{align}\]

<p>Using Eq. \ref{eqn:grad_L_b} the matrix form emerges clearly</p>

\[\begin{align}
\frac{\partial L}{\partial x} &amp;= \frac{\partial L}{\partial b} \nonumber \\
&amp;= \frac{\partial L}{\partial y}\gamma e
\end{align}\]

<h2 id="sum-all-the-streams">Sum all the Streams</h2>

<p>Now we can sum all the streams and obtain the following result without any simplification</p>

\[\begin{align}
\frac{\partial L}{\partial x} &amp;= -\frac{2}{N} (x - \mu) \left(\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma b\right) \frac{1}{d^{2}}\frac{1}{2}\frac{1}{\sqrt{c}}\right) -\frac{1}{N}\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma e\right) +\frac{\partial L}{\partial y}\gamma e
\end{align}\]

<p>Implementing these results in python:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">batchnorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="o">*</span><span class="n">cache</span><span class="p">[</span><span class="s">'x_norm'</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># Using the cache from forward
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span>
    <span class="n">mu</span> <span class="o">=</span>  <span class="n">cache</span><span class="p">[</span><span class="s">'sample_mean'</span><span class="p">]</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'sample_var'</span><span class="p">]</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'eps'</span><span class="p">]</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'beta'</span><span class="p">]</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'N'</span><span class="p">]</span>
    
    <span class="n">b</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">mu</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">+</span> <span class="n">eps</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">d</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="n">e</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">*</span><span class="n">f</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">g</span> <span class="o">+</span> <span class="n">beta</span>
    
    <span class="n">dx</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="mi">2</span><span class="o">/</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">mu</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span> <span class="n">dout</span><span class="o">*</span><span class="n">gamma</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">d</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">c</span><span class="p">))</span> <span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">N</span><span class="p">)</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span> <span class="n">dout</span><span class="o">*</span><span class="n">gamma</span><span class="o">*</span><span class="n">e</span> <span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">dout</span><span class="o">*</span><span class="n">gamma</span><span class="o">*</span><span class="n">e</span>
    
    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> 
</code></pre></div></div>

<h2 id="alternative-derivation-of-batch-normalization-backward-propogation">Alternative Derivation of Batch Normalization Backward Propogation</h2>

<p>A simpler way of calculating the backprop derivative for batch normalization is to take the derivative 
of Eq. \ref{eqn:bn}.</p>

\[\begin{align}
\frac{\partial L}{\partial x} &amp;=  \frac{\partial L}{\partial y}\frac{\partial y}{\partial x} \nonumber \\ 
&amp;= \frac{\partial L}{\partial y}\frac{\partial(\gamma \hat{x} + \beta)}{\partial x} \nonumber \\
&amp;=\frac{\partial L}{\partial y}\frac{\partial(\gamma \hat{x})}{\partial x} \nonumber \\
&amp;=\left(\frac{\partial L}{\partial y}\gamma\right)\frac{\partial\hat{x}}{\partial x}
\end{align}\]

<p>For simplicity, I will use the matrix given in Eq. \ref{eqn:xhat} for this derivation. Lets look at 
\(\frac{\partial \hat{x}}{\partial x_{0,0}}\) first to get a sense what the solution would look like:</p>

\[\begin{align}
\frac{\partial \hat{x}}{\partial x_{0,0}} &amp;= 
\begin{pmatrix}
\frac{\partial\left( \frac{x_{0,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{0,1} - \mu_{1}}{\sqrt{\sigma_{1}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{0,2} - \mu_{2}}{\sqrt{\sigma_{2}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{0,3} - \mu_{3}}{\sqrt{\sigma_{3}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{0,4} - \mu_{4}}{\sqrt{\sigma_{4}^2 + \epsilon}} \right) }{\partial x_{0,0}} \\
\frac{\partial\left( \frac{x_{1,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{1,1} - \mu_{1}}{\sqrt{\sigma_{1}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{1,2} - \mu_{2}}{\sqrt{\sigma_{2}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{1,3} - \mu_{3}}{\sqrt{\sigma_{3}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{1,4} - \mu_{4}}{\sqrt{\sigma_{4}^2 + \epsilon}} \right) }{\partial x_{0,0}} \\
\frac{\partial\left( \frac{x_{2,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{2,1} - \mu_{1}}{\sqrt{\sigma_{1}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{2,2} - \mu_{2}}{\sqrt{\sigma_{2}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{2,3} - \mu_{3}}{\sqrt{\sigma_{3}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
\frac{\partial\left( \frac{x_{2,4} - \mu_{4}}{\sqrt{\sigma_{4}^2 + \epsilon}} \right) }{\partial x_{0,0}}
\end{pmatrix}  \nonumber\\
&amp;= 
\begin{pmatrix}
\frac{\partial\left( \frac{x_{0,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
0 &amp; 
0 &amp; 
0 &amp; 
0 \\
\frac{\partial\left( \frac{x_{1,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
0 &amp; 
0 &amp; 
0 &amp; 
0 \\
\frac{\partial\left( \frac{x_{2,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} &amp; 
0 &amp; 
0 &amp; 
0 &amp; 
0
\end{pmatrix} \\
\end{align}\]

<p>Solving the first element of the matrix above using the 
<a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-9/a/quotient-rule-review" target="_blank">quotient rule</a></p>

\[\begin{align}
\frac{\partial\left( \frac{x_{0,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} = 
&amp;=\frac{ \sqrt{ \sigma_{0}^{2} + \epsilon } \frac{\partial \left(x_{0,0} - \mu_0 \right)}{\partial x_{0,0}} - 
\left(x_{0,0} - \mu_0 \right) \frac{\partial \sqrt{ \sigma_{0}^{2} + \epsilon }}{\partial x_{0,0}} }
{\left(\sqrt{\sigma_{0}^2 + \epsilon}\right)^2} \nonumber \\
&amp;=\frac{ \sqrt{ \sigma_{0}^{2} + \epsilon } \left(1 - \frac{1}{N}\right) - 
\left(x_{0,0} - \mu_0 \right)\frac{1}{2}\frac{1}{\sqrt{\sigma_{0}^{2} + \epsilon}} \frac{2}{N}( x_{0,0} - \mu_0 ) }{\sigma_{0}^2 + \epsilon} \nonumber \\
&amp;=\frac{ \sqrt{ \sigma_{0}^{2} + \epsilon } \left(1 - \frac{1}{N}\right) - 
\frac{1}{N}\frac{1}{\sqrt{\sigma_{0}^{2} + \epsilon}}\left(x_{0,0} - \mu_0 \right)( x_{0,0} - \mu_0 ) }{\sigma_{0}^2 + \epsilon}
\end{align}\]

<p>We can simlary show that</p>

\[\begin{align}
\frac{\partial\left( \frac{x_{1,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} =\frac{ \sqrt{ \sigma_{0}^{2} + \epsilon } \left(- \frac{1}{N}\right) - 
\frac{1}{N}\frac{1}{\sqrt{\sigma_{0}^{2} + \epsilon}}\left(x_{1,0} - \mu_0 \right)( x_{0,0} - \mu_0 ) }{\sigma_{0}^2 + \epsilon} \\
\frac{\partial\left( \frac{x_{2,0} - \mu_{0}}{\sqrt{\sigma_{0}^2 + \epsilon}} \right) }{\partial x_{0,0}} =\frac{ \sqrt{ \sigma_{0}^{2} + \epsilon } \left( - \frac{1}{N}\right) - 
\frac{1}{N}\frac{1}{\sqrt{\sigma_{0}^{2} + \epsilon}}\left(x_{2,0} - \mu_0 \right)( x_{0,0} - \mu_0 ) }{\sigma_{0}^2 + \epsilon}
\end{align}\]

<p>For simplicity let</p>

\[\begin{align}
v_0 = \sigma_0^2 + \epsilon
\end{align}\]

<p>The derivative \(\frac{\partial L}{\partial x_{0,0}}\) can now be given as</p>

\[\begin{align}
\frac{\partial L}{\partial x_{0,0}} =&amp; \sum_{i=0}^{2}\left(\frac{\partial L}{\partial y}\gamma\right)_{i,0}
\left(\frac{\partial \hat{x}}{\partial x_{0,0}}\right)_{i,0} \nonumber \\
=&amp;\left(\frac{\partial L}{\partial y}\gamma\right)_{0,0} \frac{ \sqrt{ v_0 } \left(1 - \frac{1}{N}\right) - 
\frac{1}{N}\frac{1}{\sqrt{v_0}}\left(x_{0,0} - \mu_0 \right)( x_{0,0} - \mu_0 ) }{v} \nonumber \\
&amp;+\left(\frac{\partial L}{\partial y}\gamma\right)_{1,0} \frac{ \sqrt{ v_0 } \left(- \frac{1}{N}\right) - 
\frac{1}{N}\frac{1}{\sqrt{v_0}}\left(x_{1,0} - \mu_0 \right)( x_{0,0} - \mu_0 ) }{v} \nonumber \\
&amp;+\left(\frac{\partial L}{\partial y}\gamma\right)_{2,0} \frac{ \sqrt{ v_0 } \left( - \frac{1}{N}\right) - 
\frac{1}{N}\frac{1}{\sqrt{v_0}}\left(x_{2,0} - \mu_0 \right)( x_{0,0} - \mu_0 ) }{v} \nonumber \\
=&amp; \left(\frac{\partial L}{\partial y}\gamma\right)_{0,0}\frac{1}{\sqrt{v_0}} + \frac{-1}{N(\sqrt{v})}\sum_{i=0}^{2}\left(\frac{\partial L}{\partial y}\gamma\right)_{i,0} \nonumber \\
&amp;+ \frac{-1}{N(v_0^{3/2})}( x_{0,0} - \mu_0 )\sum_{i=0}^{2}\left(\frac{\partial L}{\partial y}\gamma\right)_{i,0}
( x_{i,0} - \mu_0 ) \nonumber \\
=&amp; \left(\frac{\partial L}{\partial y}\gamma\right)_{0,0}\frac{1}{\sqrt{v_0}} \nonumber \\
&amp;+ \frac{1}{v_0^{3/2}} \left( \frac{-v_0}{N}\sum_{i=0}^{2}\left(\frac{\partial L}{\partial y}\gamma\right)_{i,0} + \frac{-1}{N}( x_{0,0} - \mu_0 )\sum_{i=0}^{2}\left(\frac{\partial L}{\partial y}\gamma\right)_{i,0}
( x_{i,0} - \mu_0 ) \right)
\end{align}\]

<p>We can similarly show that,</p>

\[\begin{align}
\frac{\partial L}{\partial x_{k,j}} =&amp; \left(\frac{\partial L}{\partial y}\gamma\right)_{k,j}\frac{1}{\sqrt{v_j}} \nonumber \\
&amp;+ \frac{1}{Nv_j^{3/2}} \left( -v_j\sum_{j=0}^{N-1}\left(\frac{\partial L}{\partial y}\gamma\right)_{k,j} - ( x_{k,j} - \mu_0 )\sum_{j=0}^{N-1}\left(\frac{\partial L}{\partial y}\gamma\right)_{k,j}
( x_{k,j} - \mu_j ) \right)
\end{align}\]

<p>where \(k \in N-1\) and \(j \in D-1\). We can write the result above in matrix form:</p>

\[\begin{align}
\frac{\partial L}{\partial x} =&amp; \left(\frac{\partial L}{\partial y}\gamma\right)\frac{1}{\sqrt{v}} \nonumber \\
&amp;+ \frac{1}{Nv^{3/2}} \left( -v\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma\right) - ( x - \mu )\sum^{axis=0}\left(\frac{\partial L}{\partial y}\gamma\right)
( x - \mu ) \right)
\end{align}\]

<p>This result can be implemented in python as follows:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">batchnorm_backward_alt</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
<span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">cache</span><span class="p">[</span><span class="s">'x_norm'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">v</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'sample_var'</span><span class="p">]</span> <span class="o">+</span> <span class="n">cache</span><span class="p">[</span><span class="s">'eps'</span><span class="p">]</span>

    <span class="n">dx</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">dout</span> <span class="o">/</span> <span class="n">v</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">-</span> \
                           <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">((</span><span class="n">cache</span><span class="p">[</span><span class="s">'N'</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span> <span class="o">**</span> <span class="p">(</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))))</span> <span class="o">*</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span> <span class="o">-</span> <span class="n">cache</span><span class="p">[</span><span class="s">'sample_mean'</span><span class="p">])</span>
                                                                  <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span> <span class="o">-</span> <span class="n">cache</span><span class="p">[</span><span class="s">'sample_mean'</span><span class="p">]),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="p">)</span> <span class="p">)</span>

    <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span>
</code></pre></div></div>

<p>The alternative derivative calculation is ~1.2x faster than the derivative calculated using back propogation.</p>

<h2 id="layer-normalization">Layer Normalization</h2>

<p>Layer normalization is similar to batch normalization except in layer normalization mean and variance is 
calculated over sample axis instead of feature axis. The forward pass and backward pass for layer normalization
is calculated the same way as well.</p>

<p>Layernorm forward pass:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">layernorm_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">ln_param</span><span class="p">):</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">ln_param</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">'eps'</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">)</span>
    
<span class="n">sample_mean</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">sample_var</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">x_norm</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">sample_mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sample_var</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)).</span><span class="n">T</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">x_norm</span> <span class="o">+</span> <span class="n">beta</span>

    <span class="n">cache</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">=</span> <span class="n">gamma</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'beta'</span><span class="p">]</span> <span class="o">=</span> <span class="n">beta</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'x_norm'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_norm</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'x'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'sample_mean'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_mean</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'sample_var'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sample_var</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'eps'</span><span class="p">]</span> <span class="o">=</span> <span class="n">eps</span>
    <span class="n">cache</span><span class="p">[</span><span class="s">'N'</span><span class="p">],</span> <span class="n">cache</span><span class="p">[</span><span class="s">'D'</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>
<p>The backward pass formula will be</p>

\[\begin{align}
\frac{\partial L}{\partial x} =&amp; \left(\frac{\partial L}{\partial y}\gamma\right)\frac{1}{\sqrt{v}} \nonumber \\
&amp;+ \frac{1}{Nv^{3/2}} \left( -v\sum^{axis=1}\left(\frac{\partial L}{\partial y}\gamma\right) - ( x - \mu )\sum^{axis=1}\left(\frac{\partial L}{\partial y}\gamma\right)
( x - \mu ) \right)
\end{align}\]

<p>and in Python</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">layernorm_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="n">dx</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">cache</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>

    <span class="n">dgamma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">cache</span><span class="p">[</span><span class="s">'x_norm'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">dbeta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">v</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="s">'sample_var'</span><span class="p">]</span> <span class="o">+</span> <span class="n">cache</span><span class="p">[</span><span class="s">'eps'</span><span class="p">]</span>

    <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span> <span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]</span> <span class="o">*</span> <span class="n">dout</span><span class="p">).</span><span class="n">T</span> <span class="o">/</span> <span class="n">v</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>  <span class="p">)</span> <span class="o">-</span> <span class="p">((</span><span class="mi">1</span> <span class="o">/</span> <span class="p">((</span><span class="n">cache</span><span class="p">[</span><span class="s">'D'</span><span class="p">]</span> <span class="o">*</span> <span class="n">v</span> <span class="o">**</span> <span class="p">(</span><span class="mi">3</span> <span class="o">/</span> <span class="mi">2</span><span class="p">))))</span><span class="o">*</span><span class="p">(</span>\
    <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dout</span> <span class="o">*</span> <span class="n">cache</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">v</span> <span class="o">+</span> <span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'x'</span><span class="p">].</span><span class="n">T</span> <span class="o">-</span> <span class="n">cache</span><span class="p">[</span><span class="s">'sample_mean'</span><span class="p">])</span><span class="o">*</span> \
    <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span> <span class="p">(</span><span class="n">dout</span><span class="o">*</span><span class="n">cache</span><span class="p">[</span><span class="s">'gamma'</span><span class="p">]).</span><span class="n">T</span> <span class="o">*</span> <span class="p">(</span><span class="n">cache</span><span class="p">[</span><span class="s">'x'</span><span class="p">].</span><span class="n">T</span> <span class="o">-</span> <span class="n">cache</span><span class="p">[</span><span class="s">'sample_mean'</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span> <span class="p">)</span>

    <span class="k">return</span> <span class="n">dx</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dgamma</span><span class="p">,</span> <span class="n">dbeta</span>
</code></pre></div></div>

<p>Check out the full assignment <a href="https://github.com/usmanr149/CS231n/blob/master/assignment2/BatchNormalization.ipynb" target="_blank">here</a></p>

    



<div class="post-tags">
  
    
    <a href="/urmlblog/tags.html#cs231n">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">CS231n</span>
    </a>
  
    
    <a href="/urmlblog/tags.html#neural-networks">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">Neural Networks</span>
    </a>
  
    
    <a href="/urmlblog/tags.html#batchnorm">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">Batchnorm</span>
    </a>
  
    
    <a href="/urmlblog/tags.html#batch-normalization">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">Batch Normalization</span>
    </a>
  
    
    <a href="/urmlblog/tags.html#layer-normalization">
    
      <span class="icon">
        <svg fill="#000000" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg">
    <path d="M0 0h24v24H0z" fill="none"/>
    <path d="M17.63 5.84C17.27 5.33 16.67 5 16 5L5 5.01C3.9 5.01 3 5.9 3 7v10c0 1.1.9 1.99 2 1.99L16 19c.67 0 1.27-.33 1.63-.84L22 12l-4.37-6.16z"/>
</svg>
      </span>&nbsp;<span class="tag-name">Layer Normalization</span>
    </a>
  
</div>
  </div>

  
  <section class="comments">
    <h2>Comments</h2>
    
  <div id="disqus_thread">
    <button class="disqus-load" onClick="loadDisqusComments()">
      Load Comments
    </button>
  </div>
  <script>

  /**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW
  *  TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT:s
  *  https://disqus.com/admin/universalcode/#configuration-variables
  */
  var disqus_config = function () {
    this.page.url = "http://localhost:4000/urmlblog/cs231n%20assignments/2020/04/03/Batchnorm.html";
    this.page.identifier = "" ||
                           "http://localhost:4000/urmlblog/cs231n%20assignments/2020/04/03/Batchnorm.html";
  }
  function loadDisqusComments() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//https-usmanr149-github-io-urmlblog.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  }
  </script>
  <noscript>
    Please enable JavaScript to view the
    <a href="https://disqus.com/?ref_noscript">comments powered by Disqus</a>.
  </noscript>



  </section>

  <section class="related">
  <h2>Related Posts</h2>
  <ul class="posts-list">
    
      <li>
        <h3>
          <a href="/urmlblog/computer%20vision/2022/09/09/SSD-Anchorboxes.html">
            Generating Anchor Boxes
            <small>09 Sep 2022</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/urmlblog/time%20series/2021/04/30/2021-4-30-AR-model.html">
            Autoregressive Model -- Properties of AR(1) Model
            <small>30 Apr 2021</small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/urmlblog/stocks/2020/10/01/stock-movement-prediction.html">
            Training a Machine Learning Algorithm to Predict Stock Price Movement
            <small>01 Oct 2020</small>
          </a>
        </h3>
      </li>
    
  </ul>
</section>

</div>

    </main>

    <!-- Optional footer content -->

  </body>
</html>
